apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "mlflow-v2-llama"
  namespace: "mlflow"
spec:
  predictor:
    model:
      modelFormat:
        name: mlflow
      protocolVersion: v2
      storageUri: "https://taylorsllmstorage.blob.core.windows.net/test/llama-cpp/model" 
      resources:
        limits:
          cpu: '1'
          memory: 16Gi
        requests:
          cpu: '1'
          memory: 16Gi
    tolerations:
      - key: sku
        operator: Equal
        value: gpu
        effect: NoSchedule